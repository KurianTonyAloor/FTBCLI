{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "print(run_agent_instruction(\"Create a new Git branch and switch to it\"))"
      ],
      "metadata": {
        "id": "kYr478z6vSzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_agent_instruction(instruction):\n",
        "    prompt = f\"Q: {instruction}\\nA:\"\n",
        "    response = agent_pipe(prompt)[0]['generated_text'].split(\"A:\")[-1].strip()\n",
        "\n",
        "    print(\"Instruction:\", instruction)\n",
        "    print(\"\\n--- Plan ---\")\n",
        "    for line in response.split(\"\\n\"):\n",
        "        line = line.strip()\n",
        "        print(line)\n",
        "        if line.startswith(\"`\") and line.endswith(\"`\"):\n",
        "            print(f\"[Dry-run] echo {line.strip('`')}\")\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "W-vvoMkHvMZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "def load_agent():\n",
        "    base_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "    adapter_path = \"./lora-tinyllama\"  # Your saved LoRA output directory\n",
        "\n",
        "    # Load tokenizer from base model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load base model in fp16\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16  # âœ… No quantization\n",
        "    )\n",
        "\n",
        "    # Load LoRA adapter\n",
        "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "    # Return pipeline\n",
        "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n",
        "\n",
        "agent_pipe = load_agent()\n"
      ],
      "metadata": {
        "id": "gY81-y9KvE7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate peft datasets"
      ],
      "metadata": {
        "id": "t_GTty_YrFwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes==0.41.1"
      ],
      "metadata": {
        "id": "H1-ok1VRq436"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y bitsandbytes\n",
        "!pip install bitsandbytes --upgrade --no-cache-dir"
      ],
      "metadata": {
        "id": "RhIvuRqHtlB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate peft datasets\n"
      ],
      "metadata": {
        "id": "Jhm3eoaRtwZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade bitsandbytes\n",
        "!pip install -q --upgrade transformers accelerate peft datasets"
      ],
      "metadata": {
        "id": "p6KTYrQKsJXq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}